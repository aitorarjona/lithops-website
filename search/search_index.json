{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"","title":"Home"},{"location":"getting-started/","text":"Lithops \u00b6 Lithops is a Python multicloud library for running serverless jobs. Lithops' goals are massively scaling the execution of Python code and its dependencies on serverless computing platforms and monitoring the results. Lithops delivers the user\u2019s code into the serverless platform without requiring knowledge of how functions are invoked and run. It currently supports AWS, IBM Cloud, Google Cloud, Microsoft Azure, Alibaba Aliyun, and more. Quick start \u00b6 Install Lithops from the PyPi repository: $ pip install lithops Follow choose compute backend and storage to configure Lithops. Test Lithops by simply running the next command: $ lithops test or by running the following code: import lithops def hello ( name ): return 'Hello {} !' . format ( name ) fexec = lithops . function_executor () fexec . call_async ( hello , 'World' ) print ( fexec . get_result ()) Documentation \u00b6 Website Lithops API Examples Plugins","title":"Getting started"},{"location":"getting-started/#lithops","text":"Lithops is a Python multicloud library for running serverless jobs. Lithops' goals are massively scaling the execution of Python code and its dependencies on serverless computing platforms and monitoring the results. Lithops delivers the user\u2019s code into the serverless platform without requiring knowledge of how functions are invoked and run. It currently supports AWS, IBM Cloud, Google Cloud, Microsoft Azure, Alibaba Aliyun, and more.","title":"Lithops"},{"location":"getting-started/#quick-start","text":"Install Lithops from the PyPi repository: $ pip install lithops Follow choose compute backend and storage to configure Lithops. Test Lithops by simply running the next command: $ lithops test or by running the following code: import lithops def hello ( name ): return 'Hello {} !' . format ( name ) fexec = lithops . function_executor () fexec . call_async ( hello , 'World' ) print ( fexec . get_result ())","title":"Quick start"},{"location":"getting-started/#documentation","text":"Website Lithops API Examples Plugins","title":"Documentation"},{"location":"examples/example_mandelbrot/","text":"Lithops Mandelbrot set calculation example \u00b6 Serverless matrix multiplication \u00b6 In this notebook we will calculate the Mandelbrot set on a limited space several times using Lithops. We will treat a certain region of the linear space as a matrix and we will divide it into chunks in order to be able to distribute them among many functions. For each step, we will plot the corresponding image generated from the matrix. import numpy as np from math import sqrt from matplotlib import colors from matplotlib import pyplot as plt from lithops.multiprocessing import Pool % matplotlib notebook Partitioning \u00b6 We slice the matrix into many chunks (as many as concurrency) so that each function will treat one of these. Thus, function arguments will be the limits or boundaries of a chunk. def parallel_mandelbrot ( xmin , xmax , ymin , ymax , width , height , maxiter , concurrency ): blocks_per_row = sqrt ( concurrency ) assert blocks_per_row == int ( blocks_per_row ), \"concurrency must be square number\" blocks_per_row = int ( blocks_per_row ) y_step = ( ymax - ymin ) / blocks_per_row x_step = ( xmax - xmin ) / blocks_per_row mat_block_sz = int ( width / blocks_per_row ) limits = [] indexes = [] for i in range ( blocks_per_row ): for j in range ( blocks_per_row ): limits . append (( xmin + i * x_step , xmin + ( i + 1 ) * x_step , ymin + j * y_step , ymin + ( j + 1 ) * y_step )) indexes . append (( i * mat_block_sz , ( i + 1 ) * mat_block_sz , j * mat_block_sz , ( j + 1 ) * mat_block_sz )) def mandelbrot_chunk_fn ( limit , maxiter ): rx = np . linspace ( limit [ 0 ], limit [ 1 ], mat_block_sz ) ry = np . linspace ( limit [ 2 ], limit [ 3 ], mat_block_sz ) c = rx + ry [:, None ] * 1 j output = np . zeros (( mat_block_sz , mat_block_sz )) z = np . zeros (( mat_block_sz , mat_block_sz ), np . complex64 ) for it in range ( maxiter + 1 ): notdone = np . less ( z . real * z . real + z . imag * z . imag , 4.0 ) output [ notdone ] = it z [ notdone ] = z [ notdone ] ** 2 + c [ notdone ] return output . T with Pool () as pool : iterdata = [( limit , maxiter ) for limit in limits ] results = pool . map ( mandelbrot_chunk_fn , iterdata ) mat = np . zeros (( width , height )) for i , mat_chunk in enumerate ( results ): idx = indexes [ i ] mat [ idx [ 0 ]: idx [ 1 ], idx [ 2 ]: idx [ 3 ]] = mat_chunk return mat Functions for plotting \u00b6 dpi = 72 def create_subplots ( width , height ): img_width = width / dpi img_height = height / dpi fig , ax = plt . subplots ( figsize = ( img_width , img_height ), dpi = dpi ) plt . ion () fig . show () fig . canvas . draw () return fig , ax def plot_matrix ( mat , fig , ax , xmin , xmax , ymin , ymax , width , height ): ticks = np . arange ( 0 , width , 3 * dpi ) x_ticks = xmin + ( xmax - xmin ) * ticks / width plt . xticks ( ticks , x_ticks ) y_ticks = ymin + ( ymax - ymin ) * ticks / height plt . yticks ( ticks , y_ticks ) ax . imshow ( mat . T , cmap = 'Spectral' , origin = 'lower' ) fig . canvas . draw () Execution \u00b6 We run the distributed calculation of the Mandelbrot set starting from a certain (interesting) point from space. Then, we adjust the boundaries to perform a zoom in. width = height = 768 maxiter = 300 concurrency = 16 # number of functions executed in parallel xtarget = - 0.7436438870 ytarget = 0.1318259042 # Initialize plot fig , ax = create_subplots ( width , height ) # Initial rectangle position delta = 1 xmin = xtarget - delta xmax = xtarget + delta ymin = ytarget - delta ymax = ytarget + delta mat = parallel_mandelbrot ( xmin , xmax , ymin , ymax , width , height , maxiter , concurrency ) plot_matrix ( mat , fig , ax , xmin , xmax , ymin , ymax , width , height ) # 1st zoom delta = 0.4 xmin = xtarget - delta xmax = xtarget + delta ymin = ytarget - delta ymax = ytarget + delta mat = parallel_mandelbrot ( xmin , xmax , ymin , ymax , width , height , maxiter , concurrency ) plot_matrix ( mat , fig , ax , xmin , xmax , ymin , ymax , width , height ) # 2nd zoom delta *= 0.4 xmin = xtarget - delta xmax = xtarget + delta ymin = ytarget - delta ymax = ytarget + delta maxiter += 30 mat = parallel_mandelbrot ( xmin , xmax , ymin , ymax , width , height , maxiter , concurrency ) plot_matrix ( mat , fig , ax , xmin , xmax , ymin , ymax , width , height ) # 3rd zoom delta *= 0.4 xmin = xtarget - delta xmax = xtarget + delta ymin = ytarget - delta ymax = ytarget + delta maxiter += 30 mat = parallel_mandelbrot ( xmin , xmax , ymin , ymax , width , height , maxiter , concurrency ) plot_matrix ( mat , fig , ax , xmin , xmax , ymin , ymax , width , height ) # 4th zoom delta *= 0.4 xmin = xtarget - delta xmax = xtarget + delta ymin = ytarget - delta ymax = ytarget + delta maxiter += 40 mat = parallel_mandelbrot ( xmin , xmax , ymin , ymax , width , height , maxiter , concurrency ) plot_matrix ( mat , fig , ax , xmin , xmax , ymin , ymax , width , height ) # 5th zoom delta *= 0.4 xmin = xtarget - delta xmax = xtarget + delta ymin = ytarget - delta ymax = ytarget + delta maxiter += 40 mat = parallel_mandelbrot ( xmin , xmax , ymin , ymax , width , height , maxiter , concurrency ) plot_matrix ( mat , fig , ax , xmin , xmax , ymin , ymax , width , height ) # 6th zoom delta *= 0.4 xmin = xtarget - delta xmax = xtarget + delta ymin = ytarget - delta ymax = ytarget + delta maxiter = 500 mat = parallel_mandelbrot ( xmin , xmax , ymin , ymax , width , height , maxiter , concurrency ) plot_matrix ( mat , fig , ax , xmin , xmax , ymin , ymax , width , height ) <IPython.core.display.Javascript object>","title":"Mandelbrot"},{"location":"examples/example_mandelbrot/#lithops-mandelbrot-set-calculation-example","text":"","title":"Lithops Mandelbrot set calculation example"},{"location":"examples/example_mandelbrot/#serverless-matrix-multiplication","text":"In this notebook we will calculate the Mandelbrot set on a limited space several times using Lithops. We will treat a certain region of the linear space as a matrix and we will divide it into chunks in order to be able to distribute them among many functions. For each step, we will plot the corresponding image generated from the matrix. import numpy as np from math import sqrt from matplotlib import colors from matplotlib import pyplot as plt from lithops.multiprocessing import Pool % matplotlib notebook","title":"Serverless matrix multiplication"},{"location":"examples/example_mandelbrot/#partitioning","text":"We slice the matrix into many chunks (as many as concurrency) so that each function will treat one of these. Thus, function arguments will be the limits or boundaries of a chunk. def parallel_mandelbrot ( xmin , xmax , ymin , ymax , width , height , maxiter , concurrency ): blocks_per_row = sqrt ( concurrency ) assert blocks_per_row == int ( blocks_per_row ), \"concurrency must be square number\" blocks_per_row = int ( blocks_per_row ) y_step = ( ymax - ymin ) / blocks_per_row x_step = ( xmax - xmin ) / blocks_per_row mat_block_sz = int ( width / blocks_per_row ) limits = [] indexes = [] for i in range ( blocks_per_row ): for j in range ( blocks_per_row ): limits . append (( xmin + i * x_step , xmin + ( i + 1 ) * x_step , ymin + j * y_step , ymin + ( j + 1 ) * y_step )) indexes . append (( i * mat_block_sz , ( i + 1 ) * mat_block_sz , j * mat_block_sz , ( j + 1 ) * mat_block_sz )) def mandelbrot_chunk_fn ( limit , maxiter ): rx = np . linspace ( limit [ 0 ], limit [ 1 ], mat_block_sz ) ry = np . linspace ( limit [ 2 ], limit [ 3 ], mat_block_sz ) c = rx + ry [:, None ] * 1 j output = np . zeros (( mat_block_sz , mat_block_sz )) z = np . zeros (( mat_block_sz , mat_block_sz ), np . complex64 ) for it in range ( maxiter + 1 ): notdone = np . less ( z . real * z . real + z . imag * z . imag , 4.0 ) output [ notdone ] = it z [ notdone ] = z [ notdone ] ** 2 + c [ notdone ] return output . T with Pool () as pool : iterdata = [( limit , maxiter ) for limit in limits ] results = pool . map ( mandelbrot_chunk_fn , iterdata ) mat = np . zeros (( width , height )) for i , mat_chunk in enumerate ( results ): idx = indexes [ i ] mat [ idx [ 0 ]: idx [ 1 ], idx [ 2 ]: idx [ 3 ]] = mat_chunk return mat","title":"Partitioning"},{"location":"examples/example_mandelbrot/#functions-for-plotting","text":"dpi = 72 def create_subplots ( width , height ): img_width = width / dpi img_height = height / dpi fig , ax = plt . subplots ( figsize = ( img_width , img_height ), dpi = dpi ) plt . ion () fig . show () fig . canvas . draw () return fig , ax def plot_matrix ( mat , fig , ax , xmin , xmax , ymin , ymax , width , height ): ticks = np . arange ( 0 , width , 3 * dpi ) x_ticks = xmin + ( xmax - xmin ) * ticks / width plt . xticks ( ticks , x_ticks ) y_ticks = ymin + ( ymax - ymin ) * ticks / height plt . yticks ( ticks , y_ticks ) ax . imshow ( mat . T , cmap = 'Spectral' , origin = 'lower' ) fig . canvas . draw ()","title":"Functions for plotting"},{"location":"examples/example_mandelbrot/#execution","text":"We run the distributed calculation of the Mandelbrot set starting from a certain (interesting) point from space. Then, we adjust the boundaries to perform a zoom in. width = height = 768 maxiter = 300 concurrency = 16 # number of functions executed in parallel xtarget = - 0.7436438870 ytarget = 0.1318259042 # Initialize plot fig , ax = create_subplots ( width , height ) # Initial rectangle position delta = 1 xmin = xtarget - delta xmax = xtarget + delta ymin = ytarget - delta ymax = ytarget + delta mat = parallel_mandelbrot ( xmin , xmax , ymin , ymax , width , height , maxiter , concurrency ) plot_matrix ( mat , fig , ax , xmin , xmax , ymin , ymax , width , height ) # 1st zoom delta = 0.4 xmin = xtarget - delta xmax = xtarget + delta ymin = ytarget - delta ymax = ytarget + delta mat = parallel_mandelbrot ( xmin , xmax , ymin , ymax , width , height , maxiter , concurrency ) plot_matrix ( mat , fig , ax , xmin , xmax , ymin , ymax , width , height ) # 2nd zoom delta *= 0.4 xmin = xtarget - delta xmax = xtarget + delta ymin = ytarget - delta ymax = ytarget + delta maxiter += 30 mat = parallel_mandelbrot ( xmin , xmax , ymin , ymax , width , height , maxiter , concurrency ) plot_matrix ( mat , fig , ax , xmin , xmax , ymin , ymax , width , height ) # 3rd zoom delta *= 0.4 xmin = xtarget - delta xmax = xtarget + delta ymin = ytarget - delta ymax = ytarget + delta maxiter += 30 mat = parallel_mandelbrot ( xmin , xmax , ymin , ymax , width , height , maxiter , concurrency ) plot_matrix ( mat , fig , ax , xmin , xmax , ymin , ymax , width , height ) # 4th zoom delta *= 0.4 xmin = xtarget - delta xmax = xtarget + delta ymin = ytarget - delta ymax = ytarget + delta maxiter += 40 mat = parallel_mandelbrot ( xmin , xmax , ymin , ymax , width , height , maxiter , concurrency ) plot_matrix ( mat , fig , ax , xmin , xmax , ymin , ymax , width , height ) # 5th zoom delta *= 0.4 xmin = xtarget - delta xmax = xtarget + delta ymin = ytarget - delta ymax = ytarget + delta maxiter += 40 mat = parallel_mandelbrot ( xmin , xmax , ymin , ymax , width , height , maxiter , concurrency ) plot_matrix ( mat , fig , ax , xmin , xmax , ymin , ymax , width , height ) # 6th zoom delta *= 0.4 xmin = xtarget - delta xmax = xtarget + delta ymin = ytarget - delta ymax = ytarget + delta maxiter = 500 mat = parallel_mandelbrot ( xmin , xmax , ymin , ymax , width , height , maxiter , concurrency ) plot_matrix ( mat , fig , ax , xmin , xmax , ymin , ymax , width , height ) <IPython.core.display.Javascript object>","title":"Execution"},{"location":"examples/example_mit/","text":"Lithops Moments in Time dataset example \u00b6 Video/image prediction \u00b6 In this notebook we will process video clips from the MiT dataset at scale with Lithops by predicting the actions with a pretrained ResNet50 model and then counting how many occurrences of each category have been predicted. import time import builtins import torch.optim import torch.nn.parallel from torch import save , load from torch.nn import functional as F from utils import extract_frames from models import load_model , load_transform , load_categories from lithops.multiprocessing import Pool , Queue from lithops.multiprocessing.util import get_uuid Backends \u00b6 The same program can be run in a local environtment with processes or executed by functions in the cloud. After we choose a backend, only a few file locations must be changed. In this example we will be using the cloud functions backend. We will be using a custom runtime for our functions which has torch, torchvision, ffmpeg and opencv-python modules already installed. We will store the pretrained weights in the cloud so that functions can access it. Then, after functions get the models weights they will start preprocessing input videos and inferring them one by one. Later in this notebook, we will see a little improvement detail to this process. LOCAL_EXEC = False INPUT_DATA_DIR = 'momentsintime/input_data' if LOCAL_EXEC : import os from builtins import open initargs = { 'backend' : 'localhost' , 'storage_backend' : 'localhost' } weights_location = '/dev/shm/model_weights' INPUT_DATA_DIR = os . path . abspath ( INPUT_DATA_DIR ) else : from lithops.cloud_proxy import os , open initargs = { 'backend' : 'ibm_cf' , 'storage_backend' : 'ibm_cos' , 'runtime' : 'dhak/pywren-runtime-pytorch:3.6' , 'runtime_memory' : 2048 } weights_location = 'momentsintime/models/model_weights' video_locations = [ os . path . join ( INPUT_DATA_DIR , name ) for name in os . listdir ( INPUT_DATA_DIR )] As you can see, we have masked the open function and os module with a proxy to manage files from the cloud transparently. We will use builtins.open from now on to explicitly access a local file as some accesses have to occur in the very same machine. Download pretrained ResNet50 model weights and save them in a directory accessible by all functions ( weights_location ) \u00b6 ROOT_URL = 'http://moments.csail.mit.edu/moments_models' WEIGHTS_FILE = 'moments_RGB_resnet50_imagenetpretrained.pth.tar' if not os . access ( WEIGHTS_FILE , os . R_OK ): os . system ( 'wget ' + '/' . join ([ ROOT_URL , WEIGHTS_FILE ])) with builtins . open ( WEIGHTS_FILE , 'rb' ) as f_in : weights = f_in . read () with open ( weights_location , 'wb' ) as f_out : f_out . write ( weights ) Video prediction and reduce function code \u00b6 NUM_SEGMENTS = 16 # Get dataset categories categories = load_categories () # Load the video frame transform transform = load_transform () def predict_videos ( queue , video_locations ): with open ( weights_location , 'rb' ) as f : model = load_model ( f ) model . eval () results = [] local_video_loc = 'video_to_predict_ {} .mp4' . format ( get_uuid ()) for video_loc in video_locations : start = time . time () with open ( video_loc , 'rb' ) as f_in : with builtins . open ( local_video_loc , 'wb' ) as f_out : f_out . write ( f_in . read ()) # Obtain video frames frames = extract_frames ( local_video_loc , NUM_SEGMENTS ) # Prepare input tensor [num_frames, 3, 224, 224] input_v = torch . stack ([ transform ( frame ) for frame in frames ]) # Make video prediction with torch . no_grad (): logits = model ( input_v ) h_x = F . softmax ( logits , 1 ) . mean ( dim = 0 ) probs , idx = h_x . sort ( 0 , True ) # Output the prediction result = dict ( key = video_loc ) result [ 'prediction' ] = ( idx [ 0 ], round ( float ( probs [ 0 ]), 5 )) result [ 'iter_duration' ] = time . time () - start results . append ( result ) queue . put ( results ) # Counts how many predictions of each category have been made def reduce ( queue , n ): pred_x_categ = {} for categ in categories : pred_x_categ [ categ ] = 0 checkpoint = 0.2 res_count = 0 for i in range ( n ): results = queue . get () res_count += len ( results ) for res in results : idx , prob = res [ 'prediction' ] pred_x_categ [ categories [ idx ]] += 1 # print progress if i >= ( N * checkpoint ): print ( 'Processed {} results.' . format ( res_count )) checkpoint += 0.2 return pred_x_categ Map functions \u00b6 Similar to the multiprocessing module API, we use a Pool to map the video keys across n workers (concurrency). However, we do not have to instantiate a Pool of n workers specificly , it is the map function that will invoke as many workers according to the length of the list. CONCURRENCY = 1000 queue = Queue () pool = Pool ( initargs = initargs ) # Slice data keys N = min ( CONCURRENCY , len ( video_locations )) iterable = [( queue , video_locations [ n :: CONCURRENCY ]) for n in range ( N )] # Map and reduce on the go start = time . time () pool . map_async ( func = predict_videos , iterable = iterable ) pred_x_categ = reduce ( queue , N ) end = time . time () print ( ' \\n Done.' ) print ( 'Videos processed:' , len ( video_locations )) print ( 'Total duration:' , round ( end - start , 2 ), 'sec \\n ' ) for categ , count in pred_x_categ . items (): if count != 0 : print ( ' {} : {} ' . format ( categ , count )) Performance improvement \u00b6 Now, since we know every function will have to pull the model weights from the cloud storage, we can actually pack these weights with the runtime image and reduce the start-up cost substantially. initargs [ 'runtime' ] = 'dhak/pywren-runtime-resnet' weights_location = '/momentsintime/model_weights' def predict_videos ( queue , video_locations ): # force local file access on new weights_location with builtins . open ( weights_location , 'rb' ) as f : model = load_model ( f ) model . eval () results = [] local_video_loc = 'video_to_predict_ {} .mp4' . format ( get_uuid ()) for video_loc in video_locations : start = time . time () with open ( video_loc , 'rb' ) as f_in : with builtins . open ( local_video_loc , 'wb' ) as f_out : f_out . write ( f_in . read ()) # Obtain video frames frames = extract_frames ( local_video_loc , NUM_SEGMENTS ) # Prepare input tensor [num_frames, 3, 224, 224] input_v = torch . stack ([ transform ( frame ) for frame in frames ]) # Make video prediction with torch . no_grad (): logits = model ( input_v ) h_x = F . softmax ( logits , 1 ) . mean ( dim = 0 ) probs , idx = h_x . sort ( 0 , True ) # Output the prediction result = dict ( key = video_loc ) result [ 'prediction' ] = ( idx [ 0 ], round ( float ( probs [ 0 ]), 5 )) result [ 'iter_duration' ] = time . time () - start results . append ( result ) queue . put ( results ) queue = Queue () pool = Pool ( initargs = initargs ) # Slice data keys N = min ( CONCURRENCY , len ( video_locations )) iterable = [( queue , video_locations [ n :: CONCURRENCY ]) for n in range ( N )] # Map and reduce on the go start = time . time () r = pool . map_async ( func = predict_videos , iterable = iterable ) pred_x_categ = reduce ( queue , N ) end = time . time () print ( ' \\n Done.' ) print ( 'Videos processed:' , len ( video_locations )) print ( 'Total duration:' , round ( end - start , 2 ), 'sec \\n ' ) for categ , count in pred_x_categ . items (): if count != 0 : print ( ' {} : {} ' . format ( categ , count )) Clean \u00b6 try : os . remove ( weights_location ) except FileNotFoundError : pass try : os . remove ( WEIGHTS_FILE ) except FileNotFoundError : pass Dockerfiles and build scripts for both runtimes can be found in the docker/ folder. \u00b6 Source code adapted from the demonstration in https://github.com/zhoubolei/moments_models \u00b6 Moments in Time article: http://moments.csail.mit.edu/#paper \u00b6","title":"Moments in time"},{"location":"examples/example_mit/#lithops-moments-in-time-dataset-example","text":"","title":"Lithops Moments in Time dataset example"},{"location":"examples/example_mit/#videoimage-prediction","text":"In this notebook we will process video clips from the MiT dataset at scale with Lithops by predicting the actions with a pretrained ResNet50 model and then counting how many occurrences of each category have been predicted. import time import builtins import torch.optim import torch.nn.parallel from torch import save , load from torch.nn import functional as F from utils import extract_frames from models import load_model , load_transform , load_categories from lithops.multiprocessing import Pool , Queue from lithops.multiprocessing.util import get_uuid","title":"Video/image prediction"},{"location":"examples/example_mit/#backends","text":"The same program can be run in a local environtment with processes or executed by functions in the cloud. After we choose a backend, only a few file locations must be changed. In this example we will be using the cloud functions backend. We will be using a custom runtime for our functions which has torch, torchvision, ffmpeg and opencv-python modules already installed. We will store the pretrained weights in the cloud so that functions can access it. Then, after functions get the models weights they will start preprocessing input videos and inferring them one by one. Later in this notebook, we will see a little improvement detail to this process. LOCAL_EXEC = False INPUT_DATA_DIR = 'momentsintime/input_data' if LOCAL_EXEC : import os from builtins import open initargs = { 'backend' : 'localhost' , 'storage_backend' : 'localhost' } weights_location = '/dev/shm/model_weights' INPUT_DATA_DIR = os . path . abspath ( INPUT_DATA_DIR ) else : from lithops.cloud_proxy import os , open initargs = { 'backend' : 'ibm_cf' , 'storage_backend' : 'ibm_cos' , 'runtime' : 'dhak/pywren-runtime-pytorch:3.6' , 'runtime_memory' : 2048 } weights_location = 'momentsintime/models/model_weights' video_locations = [ os . path . join ( INPUT_DATA_DIR , name ) for name in os . listdir ( INPUT_DATA_DIR )] As you can see, we have masked the open function and os module with a proxy to manage files from the cloud transparently. We will use builtins.open from now on to explicitly access a local file as some accesses have to occur in the very same machine.","title":"Backends"},{"location":"examples/example_mit/#download-pretrained-resnet50-model-weights-and-save-them-in-a-directory-accessible-by-all-functions-weights_location","text":"ROOT_URL = 'http://moments.csail.mit.edu/moments_models' WEIGHTS_FILE = 'moments_RGB_resnet50_imagenetpretrained.pth.tar' if not os . access ( WEIGHTS_FILE , os . R_OK ): os . system ( 'wget ' + '/' . join ([ ROOT_URL , WEIGHTS_FILE ])) with builtins . open ( WEIGHTS_FILE , 'rb' ) as f_in : weights = f_in . read () with open ( weights_location , 'wb' ) as f_out : f_out . write ( weights )","title":"Download pretrained ResNet50 model weights and save them in a directory accessible by all functions (weights_location)"},{"location":"examples/example_mit/#video-prediction-and-reduce-function-code","text":"NUM_SEGMENTS = 16 # Get dataset categories categories = load_categories () # Load the video frame transform transform = load_transform () def predict_videos ( queue , video_locations ): with open ( weights_location , 'rb' ) as f : model = load_model ( f ) model . eval () results = [] local_video_loc = 'video_to_predict_ {} .mp4' . format ( get_uuid ()) for video_loc in video_locations : start = time . time () with open ( video_loc , 'rb' ) as f_in : with builtins . open ( local_video_loc , 'wb' ) as f_out : f_out . write ( f_in . read ()) # Obtain video frames frames = extract_frames ( local_video_loc , NUM_SEGMENTS ) # Prepare input tensor [num_frames, 3, 224, 224] input_v = torch . stack ([ transform ( frame ) for frame in frames ]) # Make video prediction with torch . no_grad (): logits = model ( input_v ) h_x = F . softmax ( logits , 1 ) . mean ( dim = 0 ) probs , idx = h_x . sort ( 0 , True ) # Output the prediction result = dict ( key = video_loc ) result [ 'prediction' ] = ( idx [ 0 ], round ( float ( probs [ 0 ]), 5 )) result [ 'iter_duration' ] = time . time () - start results . append ( result ) queue . put ( results ) # Counts how many predictions of each category have been made def reduce ( queue , n ): pred_x_categ = {} for categ in categories : pred_x_categ [ categ ] = 0 checkpoint = 0.2 res_count = 0 for i in range ( n ): results = queue . get () res_count += len ( results ) for res in results : idx , prob = res [ 'prediction' ] pred_x_categ [ categories [ idx ]] += 1 # print progress if i >= ( N * checkpoint ): print ( 'Processed {} results.' . format ( res_count )) checkpoint += 0.2 return pred_x_categ","title":"Video prediction and reduce function code"},{"location":"examples/example_mit/#map-functions","text":"Similar to the multiprocessing module API, we use a Pool to map the video keys across n workers (concurrency). However, we do not have to instantiate a Pool of n workers specificly , it is the map function that will invoke as many workers according to the length of the list. CONCURRENCY = 1000 queue = Queue () pool = Pool ( initargs = initargs ) # Slice data keys N = min ( CONCURRENCY , len ( video_locations )) iterable = [( queue , video_locations [ n :: CONCURRENCY ]) for n in range ( N )] # Map and reduce on the go start = time . time () pool . map_async ( func = predict_videos , iterable = iterable ) pred_x_categ = reduce ( queue , N ) end = time . time () print ( ' \\n Done.' ) print ( 'Videos processed:' , len ( video_locations )) print ( 'Total duration:' , round ( end - start , 2 ), 'sec \\n ' ) for categ , count in pred_x_categ . items (): if count != 0 : print ( ' {} : {} ' . format ( categ , count ))","title":"Map functions"},{"location":"examples/example_mit/#performance-improvement","text":"Now, since we know every function will have to pull the model weights from the cloud storage, we can actually pack these weights with the runtime image and reduce the start-up cost substantially. initargs [ 'runtime' ] = 'dhak/pywren-runtime-resnet' weights_location = '/momentsintime/model_weights' def predict_videos ( queue , video_locations ): # force local file access on new weights_location with builtins . open ( weights_location , 'rb' ) as f : model = load_model ( f ) model . eval () results = [] local_video_loc = 'video_to_predict_ {} .mp4' . format ( get_uuid ()) for video_loc in video_locations : start = time . time () with open ( video_loc , 'rb' ) as f_in : with builtins . open ( local_video_loc , 'wb' ) as f_out : f_out . write ( f_in . read ()) # Obtain video frames frames = extract_frames ( local_video_loc , NUM_SEGMENTS ) # Prepare input tensor [num_frames, 3, 224, 224] input_v = torch . stack ([ transform ( frame ) for frame in frames ]) # Make video prediction with torch . no_grad (): logits = model ( input_v ) h_x = F . softmax ( logits , 1 ) . mean ( dim = 0 ) probs , idx = h_x . sort ( 0 , True ) # Output the prediction result = dict ( key = video_loc ) result [ 'prediction' ] = ( idx [ 0 ], round ( float ( probs [ 0 ]), 5 )) result [ 'iter_duration' ] = time . time () - start results . append ( result ) queue . put ( results ) queue = Queue () pool = Pool ( initargs = initargs ) # Slice data keys N = min ( CONCURRENCY , len ( video_locations )) iterable = [( queue , video_locations [ n :: CONCURRENCY ]) for n in range ( N )] # Map and reduce on the go start = time . time () r = pool . map_async ( func = predict_videos , iterable = iterable ) pred_x_categ = reduce ( queue , N ) end = time . time () print ( ' \\n Done.' ) print ( 'Videos processed:' , len ( video_locations )) print ( 'Total duration:' , round ( end - start , 2 ), 'sec \\n ' ) for categ , count in pred_x_categ . items (): if count != 0 : print ( ' {} : {} ' . format ( categ , count ))","title":"Performance improvement"},{"location":"examples/example_mit/#clean","text":"try : os . remove ( weights_location ) except FileNotFoundError : pass try : os . remove ( WEIGHTS_FILE ) except FileNotFoundError : pass","title":"Clean"},{"location":"examples/example_mit/#dockerfiles-and-build-scripts-for-both-runtimes-can-be-found-in-the-docker-folder","text":"","title":"Dockerfiles and build scripts for both runtimes can be found in the docker/ folder."},{"location":"examples/example_mit/#source-code-adapted-from-the-demonstration-in-httpsgithubcomzhouboleimoments_models","text":"","title":"Source code adapted from the demonstration in https://github.com/zhoubolei/moments_models"},{"location":"examples/example_mit/#moments-in-time-article-httpmomentscsailmitedupaper","text":"","title":"Moments in Time article: http://moments.csail.mit.edu/#paper"},{"location":"examples/example_pi_montecarlo/","text":"Lithops \u03c0 Estimation with Monte Carlo methods \u00b6 We demonstrate how to run Monte Carlo simulations with lithops over IBM Cloud Functions. This notebook contains an example of estimation the number \u03c0 with Monte Carlo. The goal of this notebook is to demonstrate how IBM Cloud Functions can benefit Monte Carlo simulations and not how it can be done using lithops. A Monte Carlo algorithm would randomly place points in the square and use the percentage of randomized points inside of the circle to estimate the value of \u03c0 Requirements to run this notebook: IBM Cloud account. Register to IBM Cloud Functions, IBM Cloud Object Storage (COS), Watson Studio You will need to have at least one existing object storage bucket. Follow COS UI to create a bucket if needed IBM Watson Studio Python notebook Step 1 - Install dependencies \u00b6 Install dependencies from time import time from random import random import logging import sys try : import lithops except : ! { sys . executable } - m pip install lithops import lithops # you can modify logging level if needed #logging.basicConfig(level=logging.INFO) Step 2 - Write Python code that implements Monte Carlo simulation \u00b6 Below is an example of Python code to demonstrate Monte Carlo model for estimate PI 'EstimatePI' is a Python class that we use to represent a single PI estimation. You may configure the following parameters: MAP_INSTANCES - number of IBM Cloud Function invocations. Default is 100 randomize_per_map - number of points to random in a single invocation. Default is 10,000,000 Our code contains two major Python methods: def randomize_points(self,data=None) - a function to random number of points and return the percentage of points that inside the circle def process_in_circle_points(self, results, futures): - summarize results of all randomize_points executions (aka \"reduce\" in map-reduce paradigm) MAP_INSTANCES = 100 class EstimatePI : randomize_per_map = 10000000 def __init__ ( self ): self . total_randomize_points = MAP_INSTANCES * self . randomize_per_map def __str__ ( self ): return \"Total Randomize Points: {:,} \" . format ( self . randomize_per_map * MAP_INSTANCES ) @staticmethod def predicate (): x = random () y = random () return ( x ** 2 ) + ( y ** 2 ) <= 1 def randomize_points ( self , data ): in_circle = 0 for _ in range ( self . randomize_per_map ): in_circle += self . predicate () return float ( in_circle / self . randomize_per_map ) def process_in_circle_points ( self , results ): in_circle_percent = 0 for map_result in results : in_circle_percent += map_result estimate_PI = float ( 4 * ( in_circle_percent / MAP_INSTANCES )) return estimate_PI Step 3 - Configure access to your COS account and Cloud Functions \u00b6 Configure access details to your IBM COS and IBM Cloud Functions. 'storage_bucket' should point to some pre-existing COS bucket. This bucket will be used by Lithops to store intermediate results. All results will be stored in the folder lithops.jobs. For additional configuration parameters see configuration section config = { 'ibm_cf' : { 'endpoint' : '<IBM Cloud Functions Endpoint>' , 'namespace' : '<NAMESPACE>' , 'api_key' : '<API KEY>' }, 'ibm_cos' : { 'endpoint' : '<IBM Cloud Object Storage Endpoint>' , 'api_key' : '<API KEY>' }, 'lithops' : { 'storage_bucket' : '<IBM COS BUCKET>' }} Step 4 - Execute simulation with Lithops over IBM Cloud Functions \u00b6 iterdata = [ 0 ] * MAP_INSTANCES est_pi = EstimatePI () start_time = time () print ( \"Monte Carlo simulation for estimating PI spawing over {} IBM Cloud Function invocations\" . format ( MAP_INSTANCES )) # obtain lithops executor pw = lithops . ibm_cf_executor ( config = config ) # execute the code pw . map_reduce ( est_pi . randomize_points , iterdata , est_pi . process_in_circle_points ) #get results result = pw . get_result () elapsed = time () print ( str ( est_pi )) print ( \"Estimation of Pi: \" , result ) print ( \" \\n Completed in: \" + str ( elapsed - start_time ) + \" seconds\" )","title":"Pi Montecarlo"},{"location":"examples/example_pi_montecarlo/#lithops-estimation-with-monte-carlo-methods","text":"We demonstrate how to run Monte Carlo simulations with lithops over IBM Cloud Functions. This notebook contains an example of estimation the number \u03c0 with Monte Carlo. The goal of this notebook is to demonstrate how IBM Cloud Functions can benefit Monte Carlo simulations and not how it can be done using lithops. A Monte Carlo algorithm would randomly place points in the square and use the percentage of randomized points inside of the circle to estimate the value of \u03c0 Requirements to run this notebook: IBM Cloud account. Register to IBM Cloud Functions, IBM Cloud Object Storage (COS), Watson Studio You will need to have at least one existing object storage bucket. Follow COS UI to create a bucket if needed IBM Watson Studio Python notebook","title":"Lithops \u03c0 Estimation with Monte Carlo methods"},{"location":"examples/example_pi_montecarlo/#step-1-install-dependencies","text":"Install dependencies from time import time from random import random import logging import sys try : import lithops except : ! { sys . executable } - m pip install lithops import lithops # you can modify logging level if needed #logging.basicConfig(level=logging.INFO)","title":"Step 1 - Install dependencies"},{"location":"examples/example_pi_montecarlo/#step-2-write-python-code-that-implements-monte-carlo-simulation","text":"Below is an example of Python code to demonstrate Monte Carlo model for estimate PI 'EstimatePI' is a Python class that we use to represent a single PI estimation. You may configure the following parameters: MAP_INSTANCES - number of IBM Cloud Function invocations. Default is 100 randomize_per_map - number of points to random in a single invocation. Default is 10,000,000 Our code contains two major Python methods: def randomize_points(self,data=None) - a function to random number of points and return the percentage of points that inside the circle def process_in_circle_points(self, results, futures): - summarize results of all randomize_points executions (aka \"reduce\" in map-reduce paradigm) MAP_INSTANCES = 100 class EstimatePI : randomize_per_map = 10000000 def __init__ ( self ): self . total_randomize_points = MAP_INSTANCES * self . randomize_per_map def __str__ ( self ): return \"Total Randomize Points: {:,} \" . format ( self . randomize_per_map * MAP_INSTANCES ) @staticmethod def predicate (): x = random () y = random () return ( x ** 2 ) + ( y ** 2 ) <= 1 def randomize_points ( self , data ): in_circle = 0 for _ in range ( self . randomize_per_map ): in_circle += self . predicate () return float ( in_circle / self . randomize_per_map ) def process_in_circle_points ( self , results ): in_circle_percent = 0 for map_result in results : in_circle_percent += map_result estimate_PI = float ( 4 * ( in_circle_percent / MAP_INSTANCES )) return estimate_PI","title":"Step 2 - Write Python code that implements Monte Carlo simulation"},{"location":"examples/example_pi_montecarlo/#step-3-configure-access-to-your-cos-account-and-cloud-functions","text":"Configure access details to your IBM COS and IBM Cloud Functions. 'storage_bucket' should point to some pre-existing COS bucket. This bucket will be used by Lithops to store intermediate results. All results will be stored in the folder lithops.jobs. For additional configuration parameters see configuration section config = { 'ibm_cf' : { 'endpoint' : '<IBM Cloud Functions Endpoint>' , 'namespace' : '<NAMESPACE>' , 'api_key' : '<API KEY>' }, 'ibm_cos' : { 'endpoint' : '<IBM Cloud Object Storage Endpoint>' , 'api_key' : '<API KEY>' }, 'lithops' : { 'storage_bucket' : '<IBM COS BUCKET>' }}","title":"Step 3 - Configure access to your COS account and Cloud Functions"},{"location":"examples/example_pi_montecarlo/#step-4-execute-simulation-with-lithops-over-ibm-cloud-functions","text":"iterdata = [ 0 ] * MAP_INSTANCES est_pi = EstimatePI () start_time = time () print ( \"Monte Carlo simulation for estimating PI spawing over {} IBM Cloud Function invocations\" . format ( MAP_INSTANCES )) # obtain lithops executor pw = lithops . ibm_cf_executor ( config = config ) # execute the code pw . map_reduce ( est_pi . randomize_points , iterdata , est_pi . process_in_circle_points ) #get results result = pw . get_result () elapsed = time () print ( str ( est_pi )) print ( \"Estimation of Pi: \" , result ) print ( \" \\n Completed in: \" + str ( elapsed - start_time ) + \" seconds\" )","title":"Step 4 - Execute simulation with Lithops over IBM Cloud Functions"}]}