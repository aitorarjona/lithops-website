{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"","title":"Home"},{"location":"getting-started/","text":"Lithops \u00b6 Lithops is a Python multicloud library for running serverless jobs It currently supports AWS, IBM Cloud, Google Cloud, Microsoft Azure, Alibaba Aliyun, and more. See plugins . Quick start \u00b6 Run functions in the cloud using the multiprocessing API: from lithops.multiprocessing import Pool def incr(x): return x + 1 pool = Pool() res = pool.map(incr, range(10)) print(res) Use cloud storage as a filesystem: from lithops.multiprocessing import Pool from lithops.cloud_proxy import os, open filename = 'bar/foo.txt' with open(filename, 'w') as f: f.write('Hello world!') dirname = os.path.dirname(filename) print(os.listdir(dirname)) def read_file(filename): with open(filename, 'r') as f: return f.read() pool = Pool() res = pool.apply(read_file, (filename,)) print(res) os.remove(filename) print(os.listdir(dirname)) Use remote in-memory cache for fast IPC and synchronization from lithops.multiprocessing import Pool, Manager, Lock from random import choice def count_chars(char, text, record, lock): count = text.count(char) record[char] = count with lock: record['total'] += count pool = Pool() record = Manager().dict() lock = Lock() # random text alphabet = 'abcdefghijklmnopqrstuvwxyz' text = ''.join([choice(alphabet) for _ in range(1000)]) record['total'] = 0 pool.map(count_chars, [(char, text, record, lock) for char in alphabet]) print(record.todict()) Documentation \u00b6 Website API Examples Plugins Use cases \u00b6 Moments in Time video prediction","title":"Getting started"},{"location":"getting-started/#lithops","text":"Lithops is a Python multicloud library for running serverless jobs It currently supports AWS, IBM Cloud, Google Cloud, Microsoft Azure, Alibaba Aliyun, and more. See plugins .","title":"Lithops"},{"location":"getting-started/#quick-start","text":"Run functions in the cloud using the multiprocessing API: from lithops.multiprocessing import Pool def incr(x): return x + 1 pool = Pool() res = pool.map(incr, range(10)) print(res) Use cloud storage as a filesystem: from lithops.multiprocessing import Pool from lithops.cloud_proxy import os, open filename = 'bar/foo.txt' with open(filename, 'w') as f: f.write('Hello world!') dirname = os.path.dirname(filename) print(os.listdir(dirname)) def read_file(filename): with open(filename, 'r') as f: return f.read() pool = Pool() res = pool.apply(read_file, (filename,)) print(res) os.remove(filename) print(os.listdir(dirname)) Use remote in-memory cache for fast IPC and synchronization from lithops.multiprocessing import Pool, Manager, Lock from random import choice def count_chars(char, text, record, lock): count = text.count(char) record[char] = count with lock: record['total'] += count pool = Pool() record = Manager().dict() lock = Lock() # random text alphabet = 'abcdefghijklmnopqrstuvwxyz' text = ''.join([choice(alphabet) for _ in range(1000)]) record['total'] = 0 pool.map(count_chars, [(char, text, record, lock) for char in alphabet]) print(record.todict())","title":"Quick start"},{"location":"getting-started/#documentation","text":"Website API Examples Plugins","title":"Documentation"},{"location":"getting-started/#use-cases","text":"Moments in Time video prediction","title":"Use cases"},{"location":"examples/example_mandelbrot/","text":"Lithops Mandelbrot set calculation example \u00b6 Serverless matrix multiplication \u00b6 In this notebook we will calculate the Mandelbrot set on a limited space several times using Lithops. We will treat a certain region of the linear space as a matrix and we will divide it into chunks in order to be able to distribute them among many functions. For each step, we will plot the corresponding image generated from the matrix. import numpy as np from math import sqrt from matplotlib import colors from matplotlib import pyplot as plt from lithops.multiprocessing import Pool % matplotlib notebook Partitioning \u00b6 We slice the matrix into many chunks (as many as concurrency) so that each function will treat one of these. Thus, function arguments will be the limits or boundaries of a chunk. def parallel_mandelbrot ( xmin , xmax , ymin , ymax , width , height , maxiter , concurrency ): blocks_per_row = sqrt ( concurrency ) assert blocks_per_row == int ( blocks_per_row ), \"concurrency must be square number\" blocks_per_row = int ( blocks_per_row ) y_step = ( ymax - ymin ) / blocks_per_row x_step = ( xmax - xmin ) / blocks_per_row mat_block_sz = int ( width / blocks_per_row ) limits = [] indexes = [] for i in range ( blocks_per_row ): for j in range ( blocks_per_row ): limits . append (( xmin + i * x_step , xmin + ( i + 1 ) * x_step , ymin + j * y_step , ymin + ( j + 1 ) * y_step )) indexes . append (( i * mat_block_sz , ( i + 1 ) * mat_block_sz , j * mat_block_sz , ( j + 1 ) * mat_block_sz )) def mandelbrot_chunk_fn ( limit , maxiter ): rx = np . linspace ( limit [ 0 ], limit [ 1 ], mat_block_sz ) ry = np . linspace ( limit [ 2 ], limit [ 3 ], mat_block_sz ) c = rx + ry [:, None ] * 1 j output = np . zeros (( mat_block_sz , mat_block_sz )) z = np . zeros (( mat_block_sz , mat_block_sz ), np . complex64 ) for it in range ( maxiter + 1 ): notdone = np . less ( z . real * z . real + z . imag * z . imag , 4.0 ) output [ notdone ] = it z [ notdone ] = z [ notdone ] ** 2 + c [ notdone ] return output . T with Pool () as pool : iterdata = [( limit , maxiter ) for limit in limits ] results = pool . map ( mandelbrot_chunk_fn , iterdata ) mat = np . zeros (( width , height )) for i , mat_chunk in enumerate ( results ): idx = indexes [ i ] mat [ idx [ 0 ]: idx [ 1 ], idx [ 2 ]: idx [ 3 ]] = mat_chunk return mat Functions for plotting \u00b6 dpi = 72 def create_subplots ( width , height ): img_width = width / dpi img_height = height / dpi fig , ax = plt . subplots ( figsize = ( img_width , img_height ), dpi = dpi ) plt . ion () fig . show () fig . canvas . draw () return fig , ax def plot_matrix ( mat , fig , ax , xmin , xmax , ymin , ymax , width , height ): ticks = np . arange ( 0 , width , 3 * dpi ) x_ticks = xmin + ( xmax - xmin ) * ticks / width plt . xticks ( ticks , x_ticks ) y_ticks = ymin + ( ymax - ymin ) * ticks / height plt . yticks ( ticks , y_ticks ) ax . imshow ( mat . T , cmap = 'Spectral' , origin = 'lower' ) fig . canvas . draw () Execution \u00b6 We run the distributed calculation of the Mandelbrot set starting from a certain (interesting) point from space. Then, we adjust the boundaries to perform a zoom in. width = height = 768 maxiter = 300 concurrency = 16 # number of functions executed in parallel xtarget = - 0.7436438870 ytarget = 0.1318259042 # Initialize plot fig , ax = create_subplots ( width , height ) # Initial rectangle position delta = 1 xmin = xtarget - delta xmax = xtarget + delta ymin = ytarget - delta ymax = ytarget + delta mat = parallel_mandelbrot ( xmin , xmax , ymin , ymax , width , height , maxiter , concurrency ) plot_matrix ( mat , fig , ax , xmin , xmax , ymin , ymax , width , height ) # 1st zoom delta = 0.4 xmin = xtarget - delta xmax = xtarget + delta ymin = ytarget - delta ymax = ytarget + delta mat = parallel_mandelbrot ( xmin , xmax , ymin , ymax , width , height , maxiter , concurrency ) plot_matrix ( mat , fig , ax , xmin , xmax , ymin , ymax , width , height ) # 2nd zoom delta *= 0.4 xmin = xtarget - delta xmax = xtarget + delta ymin = ytarget - delta ymax = ytarget + delta maxiter += 30 mat = parallel_mandelbrot ( xmin , xmax , ymin , ymax , width , height , maxiter , concurrency ) plot_matrix ( mat , fig , ax , xmin , xmax , ymin , ymax , width , height ) # 3rd zoom delta *= 0.4 xmin = xtarget - delta xmax = xtarget + delta ymin = ytarget - delta ymax = ytarget + delta maxiter += 30 mat = parallel_mandelbrot ( xmin , xmax , ymin , ymax , width , height , maxiter , concurrency ) plot_matrix ( mat , fig , ax , xmin , xmax , ymin , ymax , width , height ) # 4th zoom delta *= 0.4 xmin = xtarget - delta xmax = xtarget + delta ymin = ytarget - delta ymax = ytarget + delta maxiter += 40 mat = parallel_mandelbrot ( xmin , xmax , ymin , ymax , width , height , maxiter , concurrency ) plot_matrix ( mat , fig , ax , xmin , xmax , ymin , ymax , width , height ) # 5th zoom delta *= 0.4 xmin = xtarget - delta xmax = xtarget + delta ymin = ytarget - delta ymax = ytarget + delta maxiter += 40 mat = parallel_mandelbrot ( xmin , xmax , ymin , ymax , width , height , maxiter , concurrency ) plot_matrix ( mat , fig , ax , xmin , xmax , ymin , ymax , width , height ) # 6th zoom delta *= 0.4 xmin = xtarget - delta xmax = xtarget + delta ymin = ytarget - delta ymax = ytarget + delta maxiter = 500 mat = parallel_mandelbrot ( xmin , xmax , ymin , ymax , width , height , maxiter , concurrency ) plot_matrix ( mat , fig , ax , xmin , xmax , ymin , ymax , width , height ) <IPython.core.display.Javascript object>","title":"Mandelbrot"},{"location":"examples/example_mandelbrot/#lithops-mandelbrot-set-calculation-example","text":"","title":"Lithops Mandelbrot set calculation example"},{"location":"examples/example_mandelbrot/#serverless-matrix-multiplication","text":"In this notebook we will calculate the Mandelbrot set on a limited space several times using Lithops. We will treat a certain region of the linear space as a matrix and we will divide it into chunks in order to be able to distribute them among many functions. For each step, we will plot the corresponding image generated from the matrix. import numpy as np from math import sqrt from matplotlib import colors from matplotlib import pyplot as plt from lithops.multiprocessing import Pool % matplotlib notebook","title":"Serverless matrix multiplication"},{"location":"examples/example_mandelbrot/#partitioning","text":"We slice the matrix into many chunks (as many as concurrency) so that each function will treat one of these. Thus, function arguments will be the limits or boundaries of a chunk. def parallel_mandelbrot ( xmin , xmax , ymin , ymax , width , height , maxiter , concurrency ): blocks_per_row = sqrt ( concurrency ) assert blocks_per_row == int ( blocks_per_row ), \"concurrency must be square number\" blocks_per_row = int ( blocks_per_row ) y_step = ( ymax - ymin ) / blocks_per_row x_step = ( xmax - xmin ) / blocks_per_row mat_block_sz = int ( width / blocks_per_row ) limits = [] indexes = [] for i in range ( blocks_per_row ): for j in range ( blocks_per_row ): limits . append (( xmin + i * x_step , xmin + ( i + 1 ) * x_step , ymin + j * y_step , ymin + ( j + 1 ) * y_step )) indexes . append (( i * mat_block_sz , ( i + 1 ) * mat_block_sz , j * mat_block_sz , ( j + 1 ) * mat_block_sz )) def mandelbrot_chunk_fn ( limit , maxiter ): rx = np . linspace ( limit [ 0 ], limit [ 1 ], mat_block_sz ) ry = np . linspace ( limit [ 2 ], limit [ 3 ], mat_block_sz ) c = rx + ry [:, None ] * 1 j output = np . zeros (( mat_block_sz , mat_block_sz )) z = np . zeros (( mat_block_sz , mat_block_sz ), np . complex64 ) for it in range ( maxiter + 1 ): notdone = np . less ( z . real * z . real + z . imag * z . imag , 4.0 ) output [ notdone ] = it z [ notdone ] = z [ notdone ] ** 2 + c [ notdone ] return output . T with Pool () as pool : iterdata = [( limit , maxiter ) for limit in limits ] results = pool . map ( mandelbrot_chunk_fn , iterdata ) mat = np . zeros (( width , height )) for i , mat_chunk in enumerate ( results ): idx = indexes [ i ] mat [ idx [ 0 ]: idx [ 1 ], idx [ 2 ]: idx [ 3 ]] = mat_chunk return mat","title":"Partitioning"},{"location":"examples/example_mandelbrot/#functions-for-plotting","text":"dpi = 72 def create_subplots ( width , height ): img_width = width / dpi img_height = height / dpi fig , ax = plt . subplots ( figsize = ( img_width , img_height ), dpi = dpi ) plt . ion () fig . show () fig . canvas . draw () return fig , ax def plot_matrix ( mat , fig , ax , xmin , xmax , ymin , ymax , width , height ): ticks = np . arange ( 0 , width , 3 * dpi ) x_ticks = xmin + ( xmax - xmin ) * ticks / width plt . xticks ( ticks , x_ticks ) y_ticks = ymin + ( ymax - ymin ) * ticks / height plt . yticks ( ticks , y_ticks ) ax . imshow ( mat . T , cmap = 'Spectral' , origin = 'lower' ) fig . canvas . draw ()","title":"Functions for plotting"},{"location":"examples/example_mandelbrot/#execution","text":"We run the distributed calculation of the Mandelbrot set starting from a certain (interesting) point from space. Then, we adjust the boundaries to perform a zoom in. width = height = 768 maxiter = 300 concurrency = 16 # number of functions executed in parallel xtarget = - 0.7436438870 ytarget = 0.1318259042 # Initialize plot fig , ax = create_subplots ( width , height ) # Initial rectangle position delta = 1 xmin = xtarget - delta xmax = xtarget + delta ymin = ytarget - delta ymax = ytarget + delta mat = parallel_mandelbrot ( xmin , xmax , ymin , ymax , width , height , maxiter , concurrency ) plot_matrix ( mat , fig , ax , xmin , xmax , ymin , ymax , width , height ) # 1st zoom delta = 0.4 xmin = xtarget - delta xmax = xtarget + delta ymin = ytarget - delta ymax = ytarget + delta mat = parallel_mandelbrot ( xmin , xmax , ymin , ymax , width , height , maxiter , concurrency ) plot_matrix ( mat , fig , ax , xmin , xmax , ymin , ymax , width , height ) # 2nd zoom delta *= 0.4 xmin = xtarget - delta xmax = xtarget + delta ymin = ytarget - delta ymax = ytarget + delta maxiter += 30 mat = parallel_mandelbrot ( xmin , xmax , ymin , ymax , width , height , maxiter , concurrency ) plot_matrix ( mat , fig , ax , xmin , xmax , ymin , ymax , width , height ) # 3rd zoom delta *= 0.4 xmin = xtarget - delta xmax = xtarget + delta ymin = ytarget - delta ymax = ytarget + delta maxiter += 30 mat = parallel_mandelbrot ( xmin , xmax , ymin , ymax , width , height , maxiter , concurrency ) plot_matrix ( mat , fig , ax , xmin , xmax , ymin , ymax , width , height ) # 4th zoom delta *= 0.4 xmin = xtarget - delta xmax = xtarget + delta ymin = ytarget - delta ymax = ytarget + delta maxiter += 40 mat = parallel_mandelbrot ( xmin , xmax , ymin , ymax , width , height , maxiter , concurrency ) plot_matrix ( mat , fig , ax , xmin , xmax , ymin , ymax , width , height ) # 5th zoom delta *= 0.4 xmin = xtarget - delta xmax = xtarget + delta ymin = ytarget - delta ymax = ytarget + delta maxiter += 40 mat = parallel_mandelbrot ( xmin , xmax , ymin , ymax , width , height , maxiter , concurrency ) plot_matrix ( mat , fig , ax , xmin , xmax , ymin , ymax , width , height ) # 6th zoom delta *= 0.4 xmin = xtarget - delta xmax = xtarget + delta ymin = ytarget - delta ymax = ytarget + delta maxiter = 500 mat = parallel_mandelbrot ( xmin , xmax , ymin , ymax , width , height , maxiter , concurrency ) plot_matrix ( mat , fig , ax , xmin , xmax , ymin , ymax , width , height ) <IPython.core.display.Javascript object>","title":"Execution"},{"location":"examples/example_mit/","text":"Lithops Moments in Time dataset example \u00b6 Video/image prediction \u00b6 In this notebook we will process video clips from the MiT dataset at scale with Lithops by predicting the actions with a pretrained ResNet50 model and then counting how many occurrences of each category have been predicted. import time import builtins import torch.optim import torch.nn.parallel from torch import save , load from torch.nn import functional as F from utils import extract_frames from models import load_model , load_transform , load_categories from lithops.multiprocessing import Pool , Queue from lithops.multiprocessing.util import get_uuid Backends \u00b6 The same program can be run in a local environtment with processes or executed by functions in the cloud. After we choose a backend, only a few file locations must be changed. In this example we will be using the cloud functions backend. We will be using a custom runtime for our functions which has torch, torchvision, ffmpeg and opencv-python modules already installed. We will store the pretrained weights in the cloud so that functions can access it. Then, after functions get the models weights they will start preprocessing input videos and inferring them one by one. Later in this notebook, we will see a little improvement detail to this process. LOCAL_EXEC = False INPUT_DATA_DIR = 'momentsintime/input_data' if LOCAL_EXEC : import os from builtins import open initargs = { 'backend' : 'localhost' , 'storage_backend' : 'localhost' } weights_location = '/dev/shm/model_weights' INPUT_DATA_DIR = os . path . abspath ( INPUT_DATA_DIR ) else : from lithops.cloud_proxy import os , open initargs = { 'backend' : 'ibm_cf' , 'storage_backend' : 'ibm_cos' , 'runtime' : 'dhak/pywren-runtime-pytorch:3.6' , 'runtime_memory' : 2048 } weights_location = 'momentsintime/models/model_weights' video_locations = [ os . path . join ( INPUT_DATA_DIR , name ) for name in os . listdir ( INPUT_DATA_DIR )] As you can see, we have masked the open function and os module with a proxy to manage files from the cloud transparently. We will use builtins.open from now on to explicitly access a local file as some accesses have to occur in the very same machine. Download pretrained ResNet50 model weights and save them in a directory accessible by all functions ( weights_location ) \u00b6 ROOT_URL = 'http://moments.csail.mit.edu/moments_models' WEIGHTS_FILE = 'moments_RGB_resnet50_imagenetpretrained.pth.tar' if not os . access ( WEIGHTS_FILE , os . R_OK ): os . system ( 'wget ' + '/' . join ([ ROOT_URL , WEIGHTS_FILE ])) with builtins . open ( WEIGHTS_FILE , 'rb' ) as f_in : weights = f_in . read () with open ( weights_location , 'wb' ) as f_out : f_out . write ( weights ) Video prediction and reduce function code \u00b6 NUM_SEGMENTS = 16 # Get dataset categories categories = load_categories () # Load the video frame transform transform = load_transform () def predict_videos ( queue , video_locations ): with open ( weights_location , 'rb' ) as f : model = load_model ( f ) model . eval () results = [] local_video_loc = 'video_to_predict_ {} .mp4' . format ( get_uuid ()) for video_loc in video_locations : start = time . time () with open ( video_loc , 'rb' ) as f_in : with builtins . open ( local_video_loc , 'wb' ) as f_out : f_out . write ( f_in . read ()) # Obtain video frames frames = extract_frames ( local_video_loc , NUM_SEGMENTS ) # Prepare input tensor [num_frames, 3, 224, 224] input_v = torch . stack ([ transform ( frame ) for frame in frames ]) # Make video prediction with torch . no_grad (): logits = model ( input_v ) h_x = F . softmax ( logits , 1 ) . mean ( dim = 0 ) probs , idx = h_x . sort ( 0 , True ) # Output the prediction result = dict ( key = video_loc ) result [ 'prediction' ] = ( idx [ 0 ], round ( float ( probs [ 0 ]), 5 )) result [ 'iter_duration' ] = time . time () - start results . append ( result ) queue . put ( results ) # Counts how many predictions of each category have been made def reduce ( queue , n ): pred_x_categ = {} for categ in categories : pred_x_categ [ categ ] = 0 checkpoint = 0.2 res_count = 0 for i in range ( n ): results = queue . get () res_count += len ( results ) for res in results : idx , prob = res [ 'prediction' ] pred_x_categ [ categories [ idx ]] += 1 # print progress if i >= ( N * checkpoint ): print ( 'Processed {} results.' . format ( res_count )) checkpoint += 0.2 return pred_x_categ Map functions \u00b6 Similar to the multiprocessing module API, we use a Pool to map the video keys across n workers (concurrency). However, we do not have to instantiate a Pool of n workers specificly , it is the map function that will invoke as many workers according to the length of the list. CONCURRENCY = 1000 queue = Queue () pool = Pool ( initargs = initargs ) # Slice data keys N = min ( CONCURRENCY , len ( video_locations )) iterable = [( queue , video_locations [ n :: CONCURRENCY ]) for n in range ( N )] # Map and reduce on the go start = time . time () pool . map_async ( func = predict_videos , iterable = iterable ) pred_x_categ = reduce ( queue , N ) end = time . time () print ( ' \\n Done.' ) print ( 'Videos processed:' , len ( video_locations )) print ( 'Total duration:' , round ( end - start , 2 ), 'sec \\n ' ) for categ , count in pred_x_categ . items (): if count != 0 : print ( ' {} : {} ' . format ( categ , count )) Performance improvement \u00b6 Now, since we know every function will have to pull the model weights from the cloud storage, we can actually pack these weights with the runtime image and reduce the start-up cost substantially. initargs [ 'runtime' ] = 'dhak/pywren-runtime-resnet' weights_location = '/momentsintime/model_weights' def predict_videos ( queue , video_locations ): # force local file access on new weights_location with builtins . open ( weights_location , 'rb' ) as f : model = load_model ( f ) model . eval () results = [] local_video_loc = 'video_to_predict_ {} .mp4' . format ( get_uuid ()) for video_loc in video_locations : start = time . time () with open ( video_loc , 'rb' ) as f_in : with builtins . open ( local_video_loc , 'wb' ) as f_out : f_out . write ( f_in . read ()) # Obtain video frames frames = extract_frames ( local_video_loc , NUM_SEGMENTS ) # Prepare input tensor [num_frames, 3, 224, 224] input_v = torch . stack ([ transform ( frame ) for frame in frames ]) # Make video prediction with torch . no_grad (): logits = model ( input_v ) h_x = F . softmax ( logits , 1 ) . mean ( dim = 0 ) probs , idx = h_x . sort ( 0 , True ) # Output the prediction result = dict ( key = video_loc ) result [ 'prediction' ] = ( idx [ 0 ], round ( float ( probs [ 0 ]), 5 )) result [ 'iter_duration' ] = time . time () - start results . append ( result ) queue . put ( results ) queue = Queue () pool = Pool ( initargs = initargs ) # Slice data keys N = min ( CONCURRENCY , len ( video_locations )) iterable = [( queue , video_locations [ n :: CONCURRENCY ]) for n in range ( N )] # Map and reduce on the go start = time . time () r = pool . map_async ( func = predict_videos , iterable = iterable ) pred_x_categ = reduce ( queue , N ) end = time . time () print ( ' \\n Done.' ) print ( 'Videos processed:' , len ( video_locations )) print ( 'Total duration:' , round ( end - start , 2 ), 'sec \\n ' ) for categ , count in pred_x_categ . items (): if count != 0 : print ( ' {} : {} ' . format ( categ , count )) Clean \u00b6 try : os . remove ( weights_location ) except FileNotFoundError : pass try : os . remove ( WEIGHTS_FILE ) except FileNotFoundError : pass Dockerfiles and build scripts for both runtimes can be found in the docker/ folder. \u00b6 Source code adapted from the demonstration in https://github.com/zhoubolei/moments_models \u00b6 Moments in Time article: http://moments.csail.mit.edu/#paper \u00b6","title":"Moments in time"},{"location":"examples/example_mit/#lithops-moments-in-time-dataset-example","text":"","title":"Lithops Moments in Time dataset example"},{"location":"examples/example_mit/#videoimage-prediction","text":"In this notebook we will process video clips from the MiT dataset at scale with Lithops by predicting the actions with a pretrained ResNet50 model and then counting how many occurrences of each category have been predicted. import time import builtins import torch.optim import torch.nn.parallel from torch import save , load from torch.nn import functional as F from utils import extract_frames from models import load_model , load_transform , load_categories from lithops.multiprocessing import Pool , Queue from lithops.multiprocessing.util import get_uuid","title":"Video/image prediction"},{"location":"examples/example_mit/#backends","text":"The same program can be run in a local environtment with processes or executed by functions in the cloud. After we choose a backend, only a few file locations must be changed. In this example we will be using the cloud functions backend. We will be using a custom runtime for our functions which has torch, torchvision, ffmpeg and opencv-python modules already installed. We will store the pretrained weights in the cloud so that functions can access it. Then, after functions get the models weights they will start preprocessing input videos and inferring them one by one. Later in this notebook, we will see a little improvement detail to this process. LOCAL_EXEC = False INPUT_DATA_DIR = 'momentsintime/input_data' if LOCAL_EXEC : import os from builtins import open initargs = { 'backend' : 'localhost' , 'storage_backend' : 'localhost' } weights_location = '/dev/shm/model_weights' INPUT_DATA_DIR = os . path . abspath ( INPUT_DATA_DIR ) else : from lithops.cloud_proxy import os , open initargs = { 'backend' : 'ibm_cf' , 'storage_backend' : 'ibm_cos' , 'runtime' : 'dhak/pywren-runtime-pytorch:3.6' , 'runtime_memory' : 2048 } weights_location = 'momentsintime/models/model_weights' video_locations = [ os . path . join ( INPUT_DATA_DIR , name ) for name in os . listdir ( INPUT_DATA_DIR )] As you can see, we have masked the open function and os module with a proxy to manage files from the cloud transparently. We will use builtins.open from now on to explicitly access a local file as some accesses have to occur in the very same machine.","title":"Backends"},{"location":"examples/example_mit/#download-pretrained-resnet50-model-weights-and-save-them-in-a-directory-accessible-by-all-functions-weights_location","text":"ROOT_URL = 'http://moments.csail.mit.edu/moments_models' WEIGHTS_FILE = 'moments_RGB_resnet50_imagenetpretrained.pth.tar' if not os . access ( WEIGHTS_FILE , os . R_OK ): os . system ( 'wget ' + '/' . join ([ ROOT_URL , WEIGHTS_FILE ])) with builtins . open ( WEIGHTS_FILE , 'rb' ) as f_in : weights = f_in . read () with open ( weights_location , 'wb' ) as f_out : f_out . write ( weights )","title":"Download pretrained ResNet50 model weights and save them in a directory accessible by all functions (weights_location)"},{"location":"examples/example_mit/#video-prediction-and-reduce-function-code","text":"NUM_SEGMENTS = 16 # Get dataset categories categories = load_categories () # Load the video frame transform transform = load_transform () def predict_videos ( queue , video_locations ): with open ( weights_location , 'rb' ) as f : model = load_model ( f ) model . eval () results = [] local_video_loc = 'video_to_predict_ {} .mp4' . format ( get_uuid ()) for video_loc in video_locations : start = time . time () with open ( video_loc , 'rb' ) as f_in : with builtins . open ( local_video_loc , 'wb' ) as f_out : f_out . write ( f_in . read ()) # Obtain video frames frames = extract_frames ( local_video_loc , NUM_SEGMENTS ) # Prepare input tensor [num_frames, 3, 224, 224] input_v = torch . stack ([ transform ( frame ) for frame in frames ]) # Make video prediction with torch . no_grad (): logits = model ( input_v ) h_x = F . softmax ( logits , 1 ) . mean ( dim = 0 ) probs , idx = h_x . sort ( 0 , True ) # Output the prediction result = dict ( key = video_loc ) result [ 'prediction' ] = ( idx [ 0 ], round ( float ( probs [ 0 ]), 5 )) result [ 'iter_duration' ] = time . time () - start results . append ( result ) queue . put ( results ) # Counts how many predictions of each category have been made def reduce ( queue , n ): pred_x_categ = {} for categ in categories : pred_x_categ [ categ ] = 0 checkpoint = 0.2 res_count = 0 for i in range ( n ): results = queue . get () res_count += len ( results ) for res in results : idx , prob = res [ 'prediction' ] pred_x_categ [ categories [ idx ]] += 1 # print progress if i >= ( N * checkpoint ): print ( 'Processed {} results.' . format ( res_count )) checkpoint += 0.2 return pred_x_categ","title":"Video prediction and reduce function code"},{"location":"examples/example_mit/#map-functions","text":"Similar to the multiprocessing module API, we use a Pool to map the video keys across n workers (concurrency). However, we do not have to instantiate a Pool of n workers specificly , it is the map function that will invoke as many workers according to the length of the list. CONCURRENCY = 1000 queue = Queue () pool = Pool ( initargs = initargs ) # Slice data keys N = min ( CONCURRENCY , len ( video_locations )) iterable = [( queue , video_locations [ n :: CONCURRENCY ]) for n in range ( N )] # Map and reduce on the go start = time . time () pool . map_async ( func = predict_videos , iterable = iterable ) pred_x_categ = reduce ( queue , N ) end = time . time () print ( ' \\n Done.' ) print ( 'Videos processed:' , len ( video_locations )) print ( 'Total duration:' , round ( end - start , 2 ), 'sec \\n ' ) for categ , count in pred_x_categ . items (): if count != 0 : print ( ' {} : {} ' . format ( categ , count ))","title":"Map functions"},{"location":"examples/example_mit/#performance-improvement","text":"Now, since we know every function will have to pull the model weights from the cloud storage, we can actually pack these weights with the runtime image and reduce the start-up cost substantially. initargs [ 'runtime' ] = 'dhak/pywren-runtime-resnet' weights_location = '/momentsintime/model_weights' def predict_videos ( queue , video_locations ): # force local file access on new weights_location with builtins . open ( weights_location , 'rb' ) as f : model = load_model ( f ) model . eval () results = [] local_video_loc = 'video_to_predict_ {} .mp4' . format ( get_uuid ()) for video_loc in video_locations : start = time . time () with open ( video_loc , 'rb' ) as f_in : with builtins . open ( local_video_loc , 'wb' ) as f_out : f_out . write ( f_in . read ()) # Obtain video frames frames = extract_frames ( local_video_loc , NUM_SEGMENTS ) # Prepare input tensor [num_frames, 3, 224, 224] input_v = torch . stack ([ transform ( frame ) for frame in frames ]) # Make video prediction with torch . no_grad (): logits = model ( input_v ) h_x = F . softmax ( logits , 1 ) . mean ( dim = 0 ) probs , idx = h_x . sort ( 0 , True ) # Output the prediction result = dict ( key = video_loc ) result [ 'prediction' ] = ( idx [ 0 ], round ( float ( probs [ 0 ]), 5 )) result [ 'iter_duration' ] = time . time () - start results . append ( result ) queue . put ( results ) queue = Queue () pool = Pool ( initargs = initargs ) # Slice data keys N = min ( CONCURRENCY , len ( video_locations )) iterable = [( queue , video_locations [ n :: CONCURRENCY ]) for n in range ( N )] # Map and reduce on the go start = time . time () r = pool . map_async ( func = predict_videos , iterable = iterable ) pred_x_categ = reduce ( queue , N ) end = time . time () print ( ' \\n Done.' ) print ( 'Videos processed:' , len ( video_locations )) print ( 'Total duration:' , round ( end - start , 2 ), 'sec \\n ' ) for categ , count in pred_x_categ . items (): if count != 0 : print ( ' {} : {} ' . format ( categ , count ))","title":"Performance improvement"},{"location":"examples/example_mit/#clean","text":"try : os . remove ( weights_location ) except FileNotFoundError : pass try : os . remove ( WEIGHTS_FILE ) except FileNotFoundError : pass","title":"Clean"},{"location":"examples/example_mit/#dockerfiles-and-build-scripts-for-both-runtimes-can-be-found-in-the-docker-folder","text":"","title":"Dockerfiles and build scripts for both runtimes can be found in the docker/ folder."},{"location":"examples/example_mit/#source-code-adapted-from-the-demonstration-in-httpsgithubcomzhouboleimoments_models","text":"","title":"Source code adapted from the demonstration in https://github.com/zhoubolei/moments_models"},{"location":"examples/example_mit/#moments-in-time-article-httpmomentscsailmitedupaper","text":"","title":"Moments in Time article: http://moments.csail.mit.edu/#paper"},{"location":"examples/example_pi_montecarlo/","text":"Lithops Pi number Montecarlo approximation example \u00b6 In this notebook we will calculate an approximation to the number Pi by applying the Montecarlo algorithm with Lithops. Functions will process a fixed amount of random samples each that will then be averaged to provide the approximation. import numpy as np import matplotlib.pyplot as plt from lithops.multiprocessing import Pool , Queue % matplotlib notebook N = 1000 iter_per_func = 100000 chosen_func_mod = N // 20 num_points_to_send = 200 Monitoring \u00b6 Some functions will be chosen to send some of their already classified points over a queue that the main process will be consuming from to provide a live image of the first results. def pi_montecarlo ( n , q ): l = list () value = 0 chosen = n % chosen_func_mod == 0 for i in range ( iter_per_func ): # Generate random point between 0 and 1 x = np . random . rand () y = np . random . rand () z = np . sqrt ( x * x + y * y ) if z <= 1 : # Point is inside circle value += 1 if chosen : l . append (( x , y , z )) if i == num_points_to_send : # Send generated points q . put ( l ) chosen = False est_pi = value * 4.0 / iter_per_func return est_pi Execution \u00b6 pool = Pool () queue = Queue () results = pool . map_async ( func = pi_montecarlo , iterable = [( i , queue ) for i in range ( N )]) Cloudbutton v0.1.0 init for IBM Cloud Functions - Namespace: pol23btr%40gmail.com_dev - Region: eu_gb ExecutorID 9984cc/0 | JobID M000 - Selected Runtime: ibmfunctions/action-python-v3.6 - 256MB ExecutorID 9984cc/0 | JobID M000 - Uploading function and data - Total: 930.2KiB ExecutorID 9984cc/0 | JobID M000 - Starting function invocation: pi_montecarlo() - Total: 1000 activations Plot results on the go \u00b6 fig , ax = plt . subplots ( figsize = ( 7.5 , 7.5 )) plt . ion () fig . canvas . draw () plt . show () n_messages = 0 num_chosen_func = N // chosen_func_mod while n_messages < num_chosen_func : l = queue . get () for x , y , z in l : color = 'red' if z <= 1 else 'blue' ax . scatter ( x , y , c = color , s = 2 ) n_messages += 1 plt . title ( \"Displaying {:,d} of {:,d} generated points\" . format ( n_messages * num_points_to_send , N * iter_per_func )) fig . canvas . draw () plt . close () <IPython.core.display.Javascript object> Pi estimation \u00b6 Finally, we try to calculate the number Pi by averaging the results of each function. The more iterations we perform, the more acurate the approximation becomes. est_pi = np . mean ( results . get ()) print ( 'Estimated pi: {} ' . format ( est_pi )) print ( 'Num iterations: {:,d} ' . format ( N * iter_per_func )) ExecutorID 9984cc/0 - Getting results... HBox(children=(FloatProgress(value=0.0, max=1000.0), HTML(value=''))) Estimated pi: 3.14152828 Num iterations: 100,000,000","title":"Pi Montecarlo"},{"location":"examples/example_pi_montecarlo/#lithops-pi-number-montecarlo-approximation-example","text":"In this notebook we will calculate an approximation to the number Pi by applying the Montecarlo algorithm with Lithops. Functions will process a fixed amount of random samples each that will then be averaged to provide the approximation. import numpy as np import matplotlib.pyplot as plt from lithops.multiprocessing import Pool , Queue % matplotlib notebook N = 1000 iter_per_func = 100000 chosen_func_mod = N // 20 num_points_to_send = 200","title":"Lithops Pi number Montecarlo approximation example"},{"location":"examples/example_pi_montecarlo/#monitoring","text":"Some functions will be chosen to send some of their already classified points over a queue that the main process will be consuming from to provide a live image of the first results. def pi_montecarlo ( n , q ): l = list () value = 0 chosen = n % chosen_func_mod == 0 for i in range ( iter_per_func ): # Generate random point between 0 and 1 x = np . random . rand () y = np . random . rand () z = np . sqrt ( x * x + y * y ) if z <= 1 : # Point is inside circle value += 1 if chosen : l . append (( x , y , z )) if i == num_points_to_send : # Send generated points q . put ( l ) chosen = False est_pi = value * 4.0 / iter_per_func return est_pi","title":"Monitoring"},{"location":"examples/example_pi_montecarlo/#execution","text":"pool = Pool () queue = Queue () results = pool . map_async ( func = pi_montecarlo , iterable = [( i , queue ) for i in range ( N )]) Cloudbutton v0.1.0 init for IBM Cloud Functions - Namespace: pol23btr%40gmail.com_dev - Region: eu_gb ExecutorID 9984cc/0 | JobID M000 - Selected Runtime: ibmfunctions/action-python-v3.6 - 256MB ExecutorID 9984cc/0 | JobID M000 - Uploading function and data - Total: 930.2KiB ExecutorID 9984cc/0 | JobID M000 - Starting function invocation: pi_montecarlo() - Total: 1000 activations","title":"Execution"},{"location":"examples/example_pi_montecarlo/#plot-results-on-the-go","text":"fig , ax = plt . subplots ( figsize = ( 7.5 , 7.5 )) plt . ion () fig . canvas . draw () plt . show () n_messages = 0 num_chosen_func = N // chosen_func_mod while n_messages < num_chosen_func : l = queue . get () for x , y , z in l : color = 'red' if z <= 1 else 'blue' ax . scatter ( x , y , c = color , s = 2 ) n_messages += 1 plt . title ( \"Displaying {:,d} of {:,d} generated points\" . format ( n_messages * num_points_to_send , N * iter_per_func )) fig . canvas . draw () plt . close () <IPython.core.display.Javascript object>","title":"Plot results on the go"},{"location":"examples/example_pi_montecarlo/#pi-estimation","text":"Finally, we try to calculate the number Pi by averaging the results of each function. The more iterations we perform, the more acurate the approximation becomes. est_pi = np . mean ( results . get ()) print ( 'Estimated pi: {} ' . format ( est_pi )) print ( 'Num iterations: {:,d} ' . format ( N * iter_per_func )) ExecutorID 9984cc/0 - Getting results... HBox(children=(FloatProgress(value=0.0, max=1000.0), HTML(value=''))) Estimated pi: 3.14152828 Num iterations: 100,000,000","title":"Pi estimation"}]}